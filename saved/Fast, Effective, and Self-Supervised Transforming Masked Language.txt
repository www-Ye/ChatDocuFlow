本文提出了一种名为Mirror-BERT的训练框架，通过对输入文本进行自我复制和数据增强，将预训练的Masked Language Models (MLMs)转化为有效的通用词汇和句子编码器。实验证明，即使没有额外的数据和任务特定的微调，Mirror-BERT仍可以将MLMs转化为强大的词汇和句子编码器。在词汇级和句子级任务中，Mirror-BERT相较于原始的MLMs取得了巨大的性能提升。在句子相似度和问题-答案推理任务中，Mirror-BERT甚至可以与依赖于标注任务数据的Sentence-BERT模型相媲美。通过对MLMs的内部机理进行深入研究，我们认为Mirror-BERT之所以能够产生有效的通用词汇和句子编码器，是因为微调将MLMs中储存的丰富语义知识暴露出来。这项研究的贡献在于提供了一种简单高效的自我监督方法，可以将预训练的MLMs转化为强大的通用编码器，而无需额外的任务标注数据。
--------------------------------------------------
Mirror-BERT是一种简单、快速、自我监督且非常有效的方法，可以将大型预训练的屏蔽语言模型（MLM）转化为通用的词汇和句子编码器，不需要任何外部监督。在词汇级和句子级的语义相似性任务以及生物医学实体链接方面，Mirror-BERT通过简单的无监督数据增强技术展现出惊人的性能。在不同语言和不同脚本的任务中，相较于基础MLM模型，都获得了显著的增益。此外，我们对Mirror-BERT的有效性进行了深入分析，并探讨了其成功的主要原因。
--------------------------------------------------
Mirror-BERT是一种简单、快速、自监督且高效的方法，可以将大型预训练的遮蔽语言模型（MLM）转换为通用的词汇和句子编码器。Mirror-BERT基于简单的无监督数据增强技术，在语义相似性任务和生物医学实体链接任务中展示出出人意料的强大性能。与基础MLMs相比，Mirror-BERT在不同语言和领域之间都获得了显著的增益。此外，研究人员还分析了Mirror-BERT的有效性的主要原因。

这些段落提到了Mirror-BERT作为一种简单、快速、自监督且高效的方法，将大型预训练的遮蔽语言模型转换为通用的词汇和句子编码器。它通过简单的无监督数据增强技术展现出出人意料的强大性能，不仅适用于语义相似性任务，还适用于生物医学实体链接任务。研究人员还提到与其他类似方法相比，Mirror-BERT提供了更全面的实证结果，并在语言层面、专业生物医学领域和跨语言任务上进行了评估。最后，研究人员感谢审稿人和AC的建议，以及LTL成员和Xun Wang的反馈，同时列出了他们的论文引用和研究经费支持。
--------------------------------------------------
在这些段落中，作者介绍了使用+镜像（+Mirror）的改进方法来提高预训练语言模型的空间等价性。他们通过引入反转噪声来扰动输入，并重新训练模型以生成更加等距的表示。在多个任务上进行的实验证明，使用+Mirror调优的模型在词级和句级任务中表现出更好的结果。作者还探讨了不同的训练细节和超参数对模型性能的影响，以及+Mirror方法与其他方法（如MLM和掩码预测）之间的差异。最后，作者通过提供完整的结果表格，验证了使用+Mirror方法的有效性。
--------------------------------------------------
该段落介绍了一个配置高端的机器，采用了一个24核心处理器、两个NVIDIA GeForce RTX 2080 Ti（11 GB）的GPU和一个表格中列出的硬件规格。当遇到内存不足错误时，还会使用第二台服务器，配备两个NVIDIA GeForce RTX 3090（24 GB）的显卡。
--------------------------------------------------
span nums:5
--------------------------------------------------
==================================================
--------------------------------------------------
Mirror-BERT是一种简单、快速、自我监督且高效的方法，将大型预训练的遮蔽语言模型转换为通用的词汇和句子编码器。通过简单的无监督数据增强技术，Mirror-BERT展现出出人意料的强大性能，在语义相似性任务和生物医学实体链接任务中都表现出色。与基础MLMs相比，在不同语言和领域上都获得了显著的增益。此外，研究人员还对Mirror-BERT的有效性进行了深入分析，并探讨了其成功的主要原因。研究人员还通过提供详细的结果表格来验证了使用Mirror-BERT方法的有效性。作者还介绍了使用+Mirror来改进预训练语言模型的方法，实验证明+Mirror调优的模型在多个任务上表现更好。研究人员还讨论了不同训练细节和超参数对模型性能的影响。最后，作者还介绍了使用的高端机器配置。
--------------------------------------------------
span nums:1
--------------------------------------------------
==================================================
--------------------------------------------------
